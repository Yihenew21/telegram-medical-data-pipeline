# Telegram Medical Data Pipeline

A comprehensive ELT (Extract, Load, Transform) data pipeline that collects Telegram channel messages, stores them in a data lake, loads them into a PostgreSQL data warehouse, and transforms them into an analytical star schema using dbt.

## ğŸ† Project Highlights

âœ… **Complete ELT Pipeline**: Fully functional end-to-end data pipeline  
âœ… **Docker Orchestration**: Containerized application with PostgreSQL integration  
âœ… **dbt Transformations**: Comprehensive star schema with staging and mart layers  
âœ… **Data Quality Testing**: Built-in and custom dbt tests ensuring data integrity  
âœ… **Production-Ready**: Robust error handling, logging, and monitoring  
âœ… **Comprehensive Documentation**: Auto-generated dbt docs and detailed setup guides  

## ğŸ—ï¸ Architecture Overview

```
Telegram API â†’ Raw Data Lake â†’ PostgreSQL (Raw) â†’ dbt Transformations â†’ Analytics Schema
     â†“              â†“              â†“                    â†“
  scrape.py    JSON Files    load_raw_data.py    Star Schema Tables
```

## ğŸ“Š Data Model - Star Schema

### Dimensional Design
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   dim_channels  â”‚    â”‚   fct_messages  â”‚    â”‚   dim_dates     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ channel_pk (PK) â”‚â—„â”€â”€â”€â”¤ channel_pk (FK) â”‚â”€â”€â”€â–ºâ”‚ date_pk (PK)    â”‚
â”‚ channel_usernameâ”‚    â”‚ date_pk (FK)    â”‚    â”‚ scraped_date    â”‚
â”‚ channel_title   â”‚    â”‚ message_id      â”‚    â”‚ year            â”‚
â”‚ channel_type    â”‚    â”‚ message_text    â”‚    â”‚ month           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ views_count     â”‚    â”‚ quarter         â”‚
                       â”‚ forwards_count  â”‚    â”‚ day_of_week     â”‚
                       â”‚ replies_count   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ media_type      â”‚
                       â”‚ created_at      â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start Guide

### Prerequisites
- Docker & Docker Compose (20.10+)
- Python 3.8+
- PostgreSQL 13+ (local installation)
- Telegram API credentials ([Get them here](https://my.telegram.org))

### One-Command Setup
```bash
# Clone and setup
git clone <repository-url>
cd telegram_data_product
cp .env.example .env  # Edit with your credentials
docker-compose up --build -d

# Run complete pipeline
./run_pipeline.sh
```

### Environment Configuration

Create `.env` file with your credentials:
```env
# Telegram API Configuration
TELEGRAM_API_ID=your_api_id_here
TELEGRAM_API_HASH=your_api_hash_here  
PHONE_NUMBER=+1234567890

# PostgreSQL Configuration
POSTGRES_DB=telegram_warehouse
POSTGRES_USER=telegram_user
POSTGRES_PASSWORD=secure_password_123
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
```

### Local PostgreSQL Setup
```bash
# Create database and user
psql -U postgres
CREATE DATABASE telegram_warehouse;
CREATE USER telegram_user WITH PASSWORD 'secure_password_123';
GRANT ALL PRIVILEGES ON DATABASE telegram_warehouse TO telegram_user;
\q
```

## ğŸ”§ Pipeline Components

### 1. Data Extraction (`src/scrape.py`)
**Advanced Telegram scraping with comprehensive logging**

```python
# Target channels configuration
TARGET_CHANNELS = [
    '@CheMed123',
    '@lobelia4cosmetics', 
    '@tikvahpharma'
]
```

**Key Features:**
- âœ… Asynchronous concurrent processing
- âœ… Comprehensive media metadata extraction
- âœ… Date-based partitioning (`YYYY-MM-DD/channel_name.json`)
- âœ… Robust error handling and retry mechanisms
- âœ… Session management and authentication
- âœ… Detailed logging to `logs/scrape.log`

**Output Structure:**
```json
{
  "message_id": 123,
  "date": "2024-01-15T10:30:00+00:00",
  "text": "Sample message text",
  "views": 150,
  "forwards": 5,
  "replies": 2,
  "media_type": "photo",
  "channel_username": "channel1",
  "channel_title": "Sample Channel"
}
```

### 2. Data Loading (`src/load_raw_data.py`)
**Efficient raw data loading with incremental processing**

**Key Features:**
- âœ… Incremental loading (prevents duplicates)
- âœ… JSONB storage for flexible querying
- âœ… Transaction management with rollback
- âœ… Comprehensive error handling
- âœ… Progress tracking and logging

**Database Schema:**
```sql
CREATE SCHEMA IF NOT EXISTS raw;
CREATE TABLE IF NOT EXISTS raw.telegram_messages (
    message_id BIGINT,
    channel_username VARCHAR(255),
    scraped_date DATE,
    message_data JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (message_id, channel_username)
);
```

### 3. dbt Transformations (`telegram_dbt_project/`)
**Complete dimensional modeling with comprehensive testing**

#### ğŸ“ dbt Project Structure
```
telegram_dbt_project/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ stg_telegram_messages.sql     # Data cleaning & standardization
â”‚   â”‚   â””â”€â”€ sources.yml                   # Source definitions
â”‚   â””â”€â”€ marts/
â”‚       â”œâ”€â”€ dim_channels.sql              # Channel dimension
â”‚       â”œâ”€â”€ dim_dates.sql                 # Date dimension
â”‚       â”œâ”€â”€ fct_messages.sql              # Message fact table
â”‚       â””â”€â”€ schema.yml                    # Model tests & documentation
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ no_negative_views.sql             # Custom business rule test
â”œâ”€â”€ dbt_project.yml                       # dbt configuration
â””â”€â”€ packages.yml                          # dbt packages
```

#### ğŸ”„ Transformation Layers

##### **Staging Layer** (`models/staging/`)
```sql
-- stg_telegram_messages.sql
-- Cleans and standardizes raw JSONB data
SELECT 
    (message_data->>'message_id')::BIGINT as message_id,
    COALESCE((message_data->>'views')::INT, 0) as views_count,
    TO_TIMESTAMP(message_data->>'date', 'YYYY-MM-DD"T"HH24:MI:SS') as created_at,
    -- Additional transformations...
FROM {{ source('telegram_raw', 'telegram_messages') }}
```

##### **Mart Layer** (`models/marts/`)
- **`dim_channels`**: Unique channel information with surrogate keys
- **`dim_dates`**: Date dimension with calendar attributes  
- **`fct_messages`**: Central fact table with all metrics

#### ğŸ§ª Data Quality Testing

##### **Built-in dbt Tests**
```yaml
# models/marts/schema.yml
models:
  - name: fct_messages
    columns:
      - name: message_id
        tests:
          - unique
          - not_null
      - name: channel_pk
        tests:
          - relationships:
              to: ref('dim_channels')
              field: channel_pk
```

##### **Custom Business Rule Tests**
```sql
-- tests/no_negative_views.sql
-- Ensures views_count is never negative
SELECT *
FROM {{ ref('fct_messages') }}
WHERE views_count < 0
```

## ğŸ“Š Complete Pipeline Execution

### Automated Pipeline Script
```bash
#!/bin/bash
# run_pipeline.sh - Complete pipeline execution

echo "ğŸš€ Starting Telegram Data Pipeline..."

# Step 1: Data Extraction
echo "ğŸ“¥ Extracting data from Telegram channels..."
docker exec telegram_app python src/scrape.py

# Step 2: Data Loading  
echo "ğŸ’¾ Loading raw data to PostgreSQL..."
docker exec telegram_app python src/load_raw_data.py

# Step 3: dbt Transformations
echo "ğŸ”„ Running dbt transformations..."
docker exec telegram_app dbt run --project-dir telegram_dbt_project

# Step 4: Data Quality Tests
echo "ğŸ§ª Running data quality tests..."
docker exec telegram_app dbt test --project-dir telegram_dbt_project

# Step 5: Generate Documentation
echo "ğŸ“š Generating documentation..."
docker exec telegram_app dbt docs generate --project-dir telegram_dbt_project

echo "âœ… Pipeline completed successfully!"
```

### Manual Step-by-Step Execution
```bash
# Build and start containers
docker-compose up --build -d

# Execute pipeline steps
docker exec telegram_app python src/scrape.py
docker exec telegram_app python src/load_raw_data.py
docker exec telegram_app dbt run --project-dir telegram_dbt_project
docker exec telegram_app dbt test --project-dir telegram_dbt_project

# Generate and serve documentation
docker exec telegram_app dbt docs generate --project-dir telegram_dbt_project
docker exec telegram_app dbt docs serve --project-dir telegram_dbt_project --port 8080
```

## ğŸ” Data Quality & Testing

### Testing Strategy
1. **Built-in dbt Tests**: `unique`, `not_null`, `relationships`
2. **Custom Business Rules**: Domain-specific validation
3. **Data Freshness**: Source data recency checks
4. **Referential Integrity**: Foreign key relationships

### Test Execution
```bash
# Run all tests
docker exec telegram_app dbt test --project-dir telegram_dbt_project

# Run specific test types
docker exec telegram_app dbt test --select test_type:generic
docker exec telegram_app dbt test --select test_type:singular

# Test specific models
docker exec telegram_app dbt test --select fct_messages
```

## ğŸ“Š Analytics Examples

### Business Intelligence Queries
```sql
-- Top performing channels by engagement
SELECT 
    dc.channel_username,
    dc.channel_title,
    COUNT(*) as total_messages,
    AVG(fm.views_count) as avg_views,
    SUM(fm.forwards_count) as total_forwards
FROM analytics.fct_messages fm
JOIN analytics.dim_channels dc ON fm.channel_pk = dc.channel_pk
GROUP BY dc.channel_pk, dc.channel_username, dc.channel_title
ORDER BY avg_views DESC;

-- Daily engagement trends
SELECT 
    dd.scraped_date,
    dd.day_of_week,
    COUNT(*) as message_count,
    AVG(fm.views_count) as avg_views,
    SUM(fm.views_count) as total_views
FROM analytics.fct_messages fm
JOIN analytics.dim_dates dd ON fm.date_pk = dd.date_pk
GROUP BY dd.date_pk, dd.scraped_date, dd.day_of_week
ORDER BY dd.scraped_date DESC;
```

## ğŸ”§ Verification & Monitoring

### Data Verification Commands
```bash
# Check raw data files
ls -la data/raw/telegram_messages/

# Verify PostgreSQL data
docker exec telegram_app psql -h localhost -U telegram_user -d telegram_warehouse \
  -c "SELECT COUNT(*) FROM raw.telegram_messages;"

# Check analytics tables
docker exec telegram_app psql -h localhost -U telegram_user -d telegram_warehouse \
  -c "SELECT COUNT(*) FROM analytics.fct_messages;"
```

### Health Checks
```bash
# Container health
docker-compose ps

# Database connectivity test
docker exec telegram_app python -c "
from src.config import get_db_connection
try:
    conn = get_db_connection()
    print('âœ… Database connection successful')
    conn.close()
except Exception as e:
    print(f'âŒ Database connection failed: {e}')
"

# dbt connection test
docker exec telegram_app dbt debug --project-dir telegram_dbt_project
```

## ğŸ“š Documentation

### Auto-Generated dbt Documentation
```bash
# Generate comprehensive docs
docker exec telegram_app dbt docs generate --project-dir telegram_dbt_project

# Serve documentation locally
docker exec telegram_app dbt docs serve --project-dir telegram_dbt_project --port 8080
```

**Access documentation at:** `http://localhost:8080`

**Documentation includes:**
- ğŸ“Š Model lineage and dependencies
- ğŸ“‹ Column descriptions and data types  
- ğŸ§ª Test results and data quality metrics
- ğŸ“ˆ Model performance statistics
- ğŸ”„ Transformation logic and SQL code

## ğŸ› ï¸ Troubleshooting

### Common Issues & Solutions

#### Telegram Authentication
```bash
# Clear session and re-authenticate
rm telegram_scraper_session.session*
docker exec -it telegram_app python src/scrape.py
```

#### Database Connection Issues
```bash
# Test connection
docker exec telegram_app psql -h localhost -U telegram_user -d telegram_warehouse -c "SELECT 1;"

# Check PostgreSQL status
sudo systemctl status postgresql
```

#### dbt Issues
```bash
# Debug dbt connection
docker exec telegram_app dbt debug --project-dir telegram_dbt_project

# Check profiles configuration
docker exec telegram_app cat ~/.dbt/profiles.yml
```

## ğŸ“ Project Structure

```
telegram_data_product/
â”œâ”€â”€ src/                              # Source code
â”‚   â”œâ”€â”€ scrape.py                    # Telegram data extraction
â”‚   â”œâ”€â”€ load_raw_data.py             # PostgreSQL data loading
â”‚   â””â”€â”€ config.py                    # Configuration management
â”œâ”€â”€ telegram_dbt_project/            # dbt project (COMPLETE)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_telegram_messages.sql
â”‚   â”‚   â”‚   â””â”€â”€ sources.yml
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â”‚       â”œâ”€â”€ dim_channels.sql
â”‚   â”‚       â”œâ”€â”€ dim_dates.sql
â”‚   â”‚       â”œâ”€â”€ fct_messages.sql
â”‚   â”‚       â””â”€â”€ schema.yml
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â””â”€â”€ no_negative_views.sql
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ packages.yml
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ telegram_messages/        # Partitioned data lake
â”‚           â””â”€â”€ YYYY-MM-DD/
â”‚               â””â”€â”€ channel_name.json
â”œâ”€â”€ logs/                            # Application logs
â”‚   â”œâ”€â”€ scrape.log
â”‚   â”œâ”€â”€ load_raw_data.log
â”‚   â””â”€â”€ dbt.log
â”œâ”€â”€ docker-compose.yml               # Container orchestration
â”œâ”€â”€ Dockerfile                       # Application container
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ run_pipeline.sh                  # Automated pipeline execution
â”œâ”€â”€ .env                            # Environment variables
â”œâ”€â”€ .gitignore                      # Git ignore rules
â””â”€â”€ README.md                       # This comprehensive guide
```

## ğŸ¯ Implementation Summary

### âœ… Completed Features

1. **Project Structure & Environment**
   - Modular codebase with clear separation of concerns
   - Docker containerization with PostgreSQL integration
   - Secure credential management via `.env`
   - Comprehensive `.gitignore` configuration

2. **Data Extraction & Storage**
   - Asynchronous Telegram scraping with `telethon`
   - Partitioned data lake structure (`YYYY-MM-DD/channel.json`)
   - Robust error handling and logging
   - Media metadata extraction

3. **Data Loading & Raw Storage**
   - Incremental PostgreSQL loading
   - JSONB storage for flexible querying
   - Transaction management and error recovery
   - Duplicate prevention mechanisms

4. **dbt Transformation Pipeline**
   - Complete staging layer with data cleaning
   - Dimensional star schema (fact + dimension tables)
   - Surrogate key generation with `dbt-utils`
   - Comprehensive model documentation

5. **Data Quality & Testing**
   - Built-in dbt tests (`unique`, `not_null`, `relationships`)
   - Custom business rule tests
   - Data freshness monitoring
   - Referential integrity validation

6. **Documentation & Monitoring**
   - Auto-generated dbt documentation
   - Comprehensive README with examples
   - Health check scripts
   - Performance monitoring queries

## ğŸš€ Next Steps

1. **Execute the pipeline**: Run `./run_pipeline.sh` 
2. **Explore the data**: Use provided SQL examples
3. **View documentation**: Access `http://localhost:8080`
4. **Monitor pipeline**: Check logs and health metrics

## ğŸ“ Support

For issues or questions:
1. Check the troubleshooting section
2. Review log files in `logs/`
3. Test individual components
4. Verify environment configuration

---

**ğŸ¯ Project Status**: Production Ready âœ…  
**ğŸ“Š Pipeline Coverage**: Complete ELT Implementation  
**ğŸ† Quality Score**: All Tests Passing  
**ğŸ“š Documentation**: Comprehensive & Auto-Generated